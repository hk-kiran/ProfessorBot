{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Question\n",
    "Large Language Models perform really well on natural language generation tasks, sometimes equally or surpassing human abilities. Since GPT, natural language processing has seen many advancements, and ChatGPT showcased the power of LLMs on variety of tasks like question answering, generating long texts, and even simple logical reasoning. But, the answers were based out of the parametric memory of the model which was frozen, i.e., it could not update or fetch relevant data. Many a times the LLMs would invent results and output them: they would *hallucinate*. **Is there a way to make large language models fetch relevant information and generate accurate answers?**\n",
    "\n",
    "## The Dataset\n",
    "We are utilizing 3 chapters (chapter 1, 2, and 7) of the course textbook 'Artificial Intelligence: A Modern Approach' and a couple of slides from Prof. Keogh and Prof. LePendu as our additional context.\n",
    "\n",
    "## The Method\n",
    "To achieve our objective of eliminating hallucination and boost relevancy, we utilize Llama-Index, LangChain, and Ollama to build a RAG pipeline. Llama-Index is used to index the documents and store them in a vector database. LangChain serves the local Ollama model of Llama 2 7B. The chapters from the textbook and the slides are index and stored in a vector database which are then retrieved based on the similarity with the query passed to the language model. We have used OpenAI's state-of-the-art text-embedding-ada-002 embedding model through their API. \n",
    "\n",
    "## Our hypothesis\n",
    "We feel that supplementing the LLM with additional context can boost the relevance of the answer and stop it from hallucinating. The evaluation is done using the RAGAS approach where we measure how relevant and consistent the generated result is to the context and the query.   \n",
    "\n",
    "## Why bother?\n",
    "As the use of LLM is rising, one LLM cannot know everything in this universe. It is important to have specific LLM for a specific purpose. It is also important to have fresh data for the LLM to access and output relevant and factually correct results. The approach of RAG seems promising to alleviate most of the problems related to LLMs generation and correct usage can boost the utilization of the language models in various fields. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Jupyter Notebook for the CS205 Final Project\n",
    "*This only works if run locally after completing all the installations mentioned in the README file of the project*\n",
    "\n",
    "1. In this project we will explore a usecase of Large Language Models\n",
    "2. We will start with how to use an LLM through HuggingFace, and explain some of the basic concepts behind an LLM. \n",
    "3. Once we have a good understading of how to use an LLM for generating text, we will explore Retrieval Augmented Generation (RAG). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this project we have used Llama 2 7 Billion paramter model with OpenAI's text-embed-002 embedding model. Llama 2 7B was served locally by Ollama. We have used Llama Index and LangChain to interact with the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data store is at the root of the project directory with the name 'data'. Create a data repository before running the indexing and query cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's understand how to use an LLM using HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the same event-loop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#Import HuggingFace Transformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch Meta's OPT LLM with 1.3 billion parameters. This is quite a small model compared to the SOTA like GPT4V, etc.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('facebook/opt-1.3b')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-1.3b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Open Pre-trained Transformer (OPT) is a collection of decoder-only transformer developed by Meta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'I like CS205 Artificial Intelligence course, because'\n",
    "tok_input = tokenizer(input_text, return_tensors='pt', add_special_tokens=True, truncation=True) #Create tokens from the given input and return PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "generated_output = model.generate(**tok_input, \n",
    "                                  max_new_tokens=200, \n",
    "                                  return_dict_in_generate=True, \n",
    "                                  do_sample=True) #Generation is deterministic. \n",
    "                                                  #To use top-k sampling, set do_sample=True to get different responses in each generation\n",
    "                                                  #Set do_sample=False to have a deterministic generation each time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = tokenizer.batch_decode(generated_output.sequences, skip_special_tokens=True)[0]\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We saw text generation in the previous subsection. Now lets explore text summarization, a critical usecase of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is an embedding?\n",
    "\n",
    "Embedding is a numerical representation of the text. The words in the vocabulary are mapped to a set of integers. These integers are then converted into another mathematical representation. This 'embedding' vector is of shape (n_tokens x embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Life is sweet and sugary, but eating ice cream on Everest is long\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize1 = {s: i for i, s in enumerate(sorted(sentence.replace(',', '').split()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vec = torch.tensor([tokenize1[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence1 is tokenized as shown in the output above. The words are mapped to an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = torch.nn.Embedding(len(sentence_vec), 10)\n",
    "embedded_sentence = embed(sentence_vec).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence \"Life is sweet and sugary, but eating ice cream in Manali is long\" now has a representation like shown below. It is converted from text to a form which the machine understands. This embedding is the input to a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedded_sentence.shape)\n",
    "print(embedded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I feel like driving car today\"\n",
    "sentence2 = \"I think I want to drive today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = model.encode(sentence1)\n",
    "embedding2 = model.encode(sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a cosine similarity of 2 embeddings tells us how similar the vectors are semantically. This is particularly useful in our application where the query is matched with documents using a similarity function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.pytorch_cos_sim(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation \n",
    "\n",
    "It is a technique in generative AI to boost the knowledge of an LLM. The LLM parameters are learned and not updated to the current information, so a specialized database of knowledge (could be private) is created for the LLM is access. This is a non-parametric memory, i.e, this information is not stored in the learned paramaters of the LLM. RAG combines retrieval with generation for content generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does an RAG work?\n",
    "\n",
    "1. Retrieval: The model retrieves a set of top-k relevant documents that act as additional context for the query. Since the documents are stored in the database in the form of embeddings, the model can perform similarity searches in retrieval\n",
    "\n",
    "2. Generation: Once the documents are retrieved, this serves as additional context along with the original input. The generative model can now use both the original input and retrieved context to generated the content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing RAG with Llama Index using ChromaDB as the vector database. Ollama is used to serve Llama 2 locally\n",
    "\n",
    "The data can be accessed at this link. Add this folder to the root of the project directory\n",
    "\n",
    "https://drive.google.com/drive/folders/10wzRErO4Zlj6L3bLSqh9QtTLDkaUOuxS?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, download_loader\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = dotenv_values('../.env')[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = api_key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key # For RAGAS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the embedding\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "#embed_model = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"llama2\") #Local Llama 2 embedding model\n",
    "embed_model = OpenAIEmbedding() #Using OpenAI's text-embed-002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text-embedding-ada-002 is a powerful embedding model released by OpenAI. Like we discussed, an embedding represents a text mathematically in n-dimensions, and the distance between the embeddings can measure the similarity between the sentences or words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION = \"aiprof\"\n",
    "SLIDE_COLLECTION = 'slides'\n",
    "PATH = '../chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client and a new collection\n",
    "db = chromadb.PersistentClient(path=PATH)\n",
    "chroma_collection = db.get_or_create_collection(COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"../data/AIMA/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "db2 = chromadb.PersistentClient(path=PATH)\n",
    "chroma_collection = db2.get_or_create_collection(COLLECTION)\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index2 = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index2.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = query_engine.query(\"What is the turing test?\")\n",
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = query_engine.query(\"Generate 2 concise questions about rational agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.response.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying Slides (PPTx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client and a new collection\n",
    "slides_db = chromadb.PersistentClient(path=PATH)\n",
    "slides_chroma_collection = slides_db.get_or_create_collection(SLIDE_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=slides_chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slides_reader = download_loader(\"PptxReader\")\n",
    "loader = slides_reader()\n",
    "\n",
    "print(type(SimpleDirectoryReader))\n",
    "\n",
    "slides = loader.load_data(Path('../data/slides/4_Adversarial_Search.pptx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slides_index = VectorStoreIndex.from_documents(documents=slides, \n",
    "                                               storage_context=storage_context, \n",
    "                                               service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slides_query = slides_index.as_query_engine()\n",
    "resp = slides_query.query(\"What is The Minimax Algorithm?\")\n",
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We have been quite successful at getting what looks like accurate results, but lets run some evaluations on the generated text. Inspired from the RAGAS paper (https://arxiv.org/pdf/2309.15217v1.pdf), there is a ragas python library which runs evaluations on the RAG generated text. \n",
    "\n",
    "**Retrieval Augmentation Generated Assessment** evaluates a RAG generated text without human interference. Even after utilizing RAG we wouldn't be sure if the system hallucinated the generation. The paper proposes a suite of metrics that evaluates RAG.\n",
    "\n",
    "*Faithfulness*: Measures the factual consistency of the generated answer. It is based on the answer and the retrieved context. The generated answer is faithful if all the answers can be deduced from the given context. The score ranges from 0 to 1. Higher the score, the better. \n",
    "\n",
    "*Answer Relevance*: Measures how relevant the answer is to the given prompt. Incomplete and/or redudant answers are given a lower score. The score ranges from 0 to 1.   \n",
    "\n",
    "*Context Relevance*: Similar to answer relevancy, context relevancy measures the relevance of the generated text to the context. The idea is that the context should exlusively contain the information required to answer the query prompt. \n",
    "\n",
    "*Context Precision*: Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. This metric is computed using the question and the contexts, with values ranging between 0 and 1, where higher scores indicate better precision.\n",
    "\n",
    "*Context Recall*: Context recall measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. It is computed based on the ground truth and the retrieved context, and the values range between 0 and 1, with higher values indicating better performance.\n",
    "\n",
    "Definitions are sourced from https://github.com/explodinggradients/ragas/tree/main/docs/concepts/metrics and the RAGAS paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    harmfulness,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llama_index import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\"What is the turing test?\", \n",
    "                  \"What is the rational agent approach?\"]\n",
    "\n",
    "eval_answers = [\"The Turing test, originally called the imitation game by Alan Turing in 1950,[2] is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses.\",\n",
    "                \"\"]\n",
    "\n",
    "eval_answers = [[a] for a in eval_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:24<00:00, 204.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 1/1 [00:37<00:00, 37.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [harmfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 1/1 [00:34<00:00, 34.74s/it]\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(query_engine, metrics, eval_questions, eval_answers) #Takes long to run. 5-10min depending on the number of questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>harmfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the turing test?</td>\n",
       "      <td>[6 Chapter 1.Introduction\\ntheso-called totalT...</td>\n",
       "      <td>The Turing Test is a theoretical framework for...</td>\n",
       "      <td>[The Turing test, originally called the imitat...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.920805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   question  \\\n",
       "0  What is the turing test?   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [6 Chapter 1.Introduction\\ntheso-called totalT...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The Turing Test is a theoretical framework for...   \n",
       "\n",
       "                                       ground_truths  faithfulness  \\\n",
       "0  [The Turing test, originally called the imitat...      0.285714   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  harmfulness  \n",
       "0          0.920805                0.0             1.0            0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
